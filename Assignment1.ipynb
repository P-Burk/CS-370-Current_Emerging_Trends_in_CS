{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_239 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 1.4982 - accuracy: 0.6160 - val_loss: 0.7603 - val_accuracy: 0.8422\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.5970 - accuracy: 0.8563 - val_loss: 0.4521 - val_accuracy: 0.8830\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.4350 - accuracy: 0.8835 - val_loss: 0.3722 - val_accuracy: 0.8981\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.3748 - accuracy: 0.8958 - val_loss: 0.3352 - val_accuracy: 0.9050\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.3410 - accuracy: 0.9048 - val_loss: 0.3100 - val_accuracy: 0.9136\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.3178 - accuracy: 0.9102 - val_loss: 0.2926 - val_accuracy: 0.9166\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2998 - accuracy: 0.9146 - val_loss: 0.2780 - val_accuracy: 0.9211\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2852 - accuracy: 0.9191 - val_loss: 0.2660 - val_accuracy: 0.9245\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2731 - accuracy: 0.9221 - val_loss: 0.2560 - val_accuracy: 0.9283\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2622 - accuracy: 0.9254 - val_loss: 0.2488 - val_accuracy: 0.9287\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.2523 - accuracy: 0.9278 - val_loss: 0.2398 - val_accuracy: 0.9317\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2431 - accuracy: 0.9305 - val_loss: 0.2323 - val_accuracy: 0.9339\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.2348 - accuracy: 0.9326 - val_loss: 0.2280 - val_accuracy: 0.9355\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2274 - accuracy: 0.9349 - val_loss: 0.2198 - val_accuracy: 0.9374\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.2200 - accuracy: 0.9373 - val_loss: 0.2128 - val_accuracy: 0.9392\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2134 - accuracy: 0.9385 - val_loss: 0.2074 - val_accuracy: 0.9415\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2071 - accuracy: 0.9403 - val_loss: 0.2020 - val_accuracy: 0.9427\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2011 - accuracy: 0.9428 - val_loss: 0.1975 - val_accuracy: 0.9434\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.1956 - accuracy: 0.9439 - val_loss: 0.1938 - val_accuracy: 0.9454\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1905 - accuracy: 0.9451 - val_loss: 0.1885 - val_accuracy: 0.9463\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "Test score: 0.18818295242637395\n",
      "Test accuracy: 0.9467999935150146\n",
      "Total time: 0:00:35.076000\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_248 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.7455 - accuracy: 0.8077 - val_loss: 0.3395 - val_accuracy: 0.9064\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.3243 - accuracy: 0.9079 - val_loss: 0.2728 - val_accuracy: 0.9208\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2704 - accuracy: 0.9228 - val_loss: 0.2384 - val_accuracy: 0.9320\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.2366 - accuracy: 0.9322 - val_loss: 0.2134 - val_accuracy: 0.9396\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2112 - accuracy: 0.9388 - val_loss: 0.1942 - val_accuracy: 0.9459\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.1913 - accuracy: 0.9452 - val_loss: 0.1811 - val_accuracy: 0.9481\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.1739 - accuracy: 0.9497 - val_loss: 0.1692 - val_accuracy: 0.9511\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.1587 - accuracy: 0.9541 - val_loss: 0.1581 - val_accuracy: 0.9542\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.1465 - accuracy: 0.9581 - val_loss: 0.1450 - val_accuracy: 0.9584\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.1352 - accuracy: 0.9616 - val_loss: 0.1391 - val_accuracy: 0.9605\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1252 - accuracy: 0.9643 - val_loss: 0.1324 - val_accuracy: 0.9626\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.1164 - accuracy: 0.9661 - val_loss: 0.1270 - val_accuracy: 0.9631\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1090 - accuracy: 0.9688 - val_loss: 0.1288 - val_accuracy: 0.9627\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.1022 - accuracy: 0.9708 - val_loss: 0.1190 - val_accuracy: 0.9647\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.0956 - accuracy: 0.9729 - val_loss: 0.1133 - val_accuracy: 0.9669\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.0903 - accuracy: 0.9740 - val_loss: 0.1099 - val_accuracy: 0.9687\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.0849 - accuracy: 0.9762 - val_loss: 0.1052 - val_accuracy: 0.9697\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.0803 - accuracy: 0.9772 - val_loss: 0.1033 - val_accuracy: 0.9691\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.0759 - accuracy: 0.9790 - val_loss: 0.1026 - val_accuracy: 0.9702\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.0721 - accuracy: 0.9804 - val_loss: 0.1007 - val_accuracy: 0.9697\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "Test score: 0.09466224124152213\n",
      "Test accuracy: 0.9707000255584717\n",
      "Total time: 0:02:01.575995\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_251 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 1.8665 - accuracy: 0.4834 - val_loss: 1.3338 - val_accuracy: 0.7286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 1.0110 - accuracy: 0.7769 - val_loss: 0.7353 - val_accuracy: 0.8384\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.6598 - accuracy: 0.8372 - val_loss: 0.5383 - val_accuracy: 0.8683\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.5245 - accuracy: 0.8643 - val_loss: 0.4497 - val_accuracy: 0.8830\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4553 - accuracy: 0.8789 - val_loss: 0.4005 - val_accuracy: 0.8944\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4131 - accuracy: 0.8881 - val_loss: 0.3695 - val_accuracy: 0.8992\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3844 - accuracy: 0.8942 - val_loss: 0.3468 - val_accuracy: 0.9041\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3632 - accuracy: 0.8993 - val_loss: 0.3302 - val_accuracy: 0.9080\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3470 - accuracy: 0.9035 - val_loss: 0.3174 - val_accuracy: 0.9109\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3337 - accuracy: 0.9066 - val_loss: 0.3071 - val_accuracy: 0.9128\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3225 - accuracy: 0.9087 - val_loss: 0.2976 - val_accuracy: 0.9154\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3127 - accuracy: 0.9111 - val_loss: 0.2897 - val_accuracy: 0.9184\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3041 - accuracy: 0.9139 - val_loss: 0.2841 - val_accuracy: 0.9184\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2966 - accuracy: 0.9157 - val_loss: 0.2765 - val_accuracy: 0.9208\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2896 - accuracy: 0.9178 - val_loss: 0.2703 - val_accuracy: 0.9227\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2831 - accuracy: 0.9196 - val_loss: 0.2650 - val_accuracy: 0.9240\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2772 - accuracy: 0.9211 - val_loss: 0.2602 - val_accuracy: 0.9243\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2716 - accuracy: 0.9230 - val_loss: 0.2558 - val_accuracy: 0.9261\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2665 - accuracy: 0.9240 - val_loss: 0.2513 - val_accuracy: 0.9271\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2618 - accuracy: 0.9253 - val_loss: 0.2475 - val_accuracy: 0.9284\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "Test score: 0.25154674277305605\n",
      "Test accuracy: 0.9277999997138977\n",
      "Total time: 0:00:19.938010\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 256\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_274 (Dense)            (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_276 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_278 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_280 (Dense)            (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 532,490\n",
      "Trainable params: 532,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 1.3298 - accuracy: 0.6500 - val_loss: 0.4373 - val_accuracy: 0.8641\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3576 - accuracy: 0.8942 - val_loss: 0.2825 - val_accuracy: 0.9160\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2659 - accuracy: 0.9209 - val_loss: 0.2373 - val_accuracy: 0.9297\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.2161 - accuracy: 0.9364 - val_loss: 0.1874 - val_accuracy: 0.9468\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.1821 - accuracy: 0.9464 - val_loss: 0.1679 - val_accuracy: 0.9503\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.1574 - accuracy: 0.9530 - val_loss: 0.1577 - val_accuracy: 0.9528\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.1383 - accuracy: 0.9583 - val_loss: 0.1433 - val_accuracy: 0.9585\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.1226 - accuracy: 0.9634 - val_loss: 0.1307 - val_accuracy: 0.9619\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.1092 - accuracy: 0.9674 - val_loss: 0.1246 - val_accuracy: 0.9644\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0980 - accuracy: 0.9713 - val_loss: 0.1164 - val_accuracy: 0.9652\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0881 - accuracy: 0.9738 - val_loss: 0.1146 - val_accuracy: 0.9660\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0793 - accuracy: 0.9769 - val_loss: 0.1120 - val_accuracy: 0.9669\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0716 - accuracy: 0.9789 - val_loss: 0.1062 - val_accuracy: 0.9686\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.0651 - accuracy: 0.9810 - val_loss: 0.1127 - val_accuracy: 0.9663\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0582 - accuracy: 0.9830 - val_loss: 0.1062 - val_accuracy: 0.9681\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0529 - accuracy: 0.9846 - val_loss: 0.1058 - val_accuracy: 0.9693\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.0475 - accuracy: 0.9861 - val_loss: 0.1049 - val_accuracy: 0.9710\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.0427 - accuracy: 0.9882 - val_loss: 0.0979 - val_accuracy: 0.9728\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0386 - accuracy: 0.9889 - val_loss: 0.1034 - val_accuracy: 0.9706\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0343 - accuracy: 0.9907 - val_loss: 0.1019 - val_accuracy: 0.9718\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0310 - accuracy: 0.9915 - val_loss: 0.1024 - val_accuracy: 0.9721\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0277 - accuracy: 0.9925 - val_loss: 0.1016 - val_accuracy: 0.9729\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 0.1113 - val_accuracy: 0.9708\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.0217 - accuracy: 0.9948 - val_loss: 0.1158 - val_accuracy: 0.9688\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1055 - val_accuracy: 0.9726\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.1070 - val_accuracy: 0.9722\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.0147 - accuracy: 0.9967 - val_loss: 0.1115 - val_accuracy: 0.9725\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.0128 - accuracy: 0.9975 - val_loss: 0.1222 - val_accuracy: 0.9696\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.1119 - val_accuracy: 0.9722\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0100 - accuracy: 0.9982 - val_loss: 0.1172 - val_accuracy: 0.9727\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "Test score: 0.08998867263757857\n",
      "Test accuracy: 0.9753999710083008\n",
      "Total time: 0:01:49.188845\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 30\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 256 #added more nodes\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d020614f9a2dc57296da84a5f935b826668275142c238b2790968ef78e3430c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
