{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Five Assignment: Cartpole Problem\n",
    "Review the code in this notebook and in the score_logger.py file in the *scores* folder (directory). Once you have reviewed the code, return to this notebook and select **Cell** and then **Run All** from the menu bar to run this code. The code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "\n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "  \n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "  \n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "  \n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 0.8433051360508336, score: 54\n",
      "Scores: (min: 54, avg: 54, max: 54)\n",
      "\n",
      "Run: 2, exploration: 0.7666961448653229, score: 20\n",
      "Scores: (min: 20, avg: 37, max: 54)\n",
      "\n",
      "Run: 3, exploration: 0.736559652908221, score: 9\n",
      "Scores: (min: 9, avg: 27.666666666666668, max: 54)\n",
      "\n",
      "Run: 4, exploration: 0.6900935609921609, score: 14\n",
      "Scores: (min: 9, avg: 14.333333333333334, max: 20)\n",
      "\n",
      "Run: 5, exploration: 0.6563549768288433, score: 11\n",
      "Scores: (min: 9, avg: 11.333333333333334, max: 14)\n",
      "\n",
      "Run: 6, exploration: 0.6242658676435396, score: 11\n",
      "Scores: (min: 11, avg: 12, max: 14)\n",
      "\n",
      "Run: 7, exploration: 0.5907768628656763, score: 12\n",
      "Scores: (min: 11, avg: 11.333333333333334, max: 12)\n",
      "\n",
      "Run: 8, exploration: 0.5647174463480732, score: 10\n",
      "Scores: (min: 10, avg: 11, max: 12)\n",
      "\n",
      "Run: 9, exploration: 0.5371084840724134, score: 11\n",
      "Scores: (min: 10, avg: 11, max: 12)\n",
      "\n",
      "Run: 10, exploration: 0.5159963842937159, score: 9\n",
      "Scores: (min: 9, avg: 10, max: 11)\n",
      "\n",
      "Run: 11, exploration: 0.4738479773082268, score: 18\n",
      "Scores: (min: 9, avg: 12.666666666666666, max: 18)\n",
      "\n",
      "Run: 12, exploration: 0.43732904629000013, score: 17\n",
      "Scores: (min: 9, avg: 14.666666666666666, max: 18)\n",
      "\n",
      "Run: 13, exploration: 0.3897078735047413, score: 24\n",
      "Scores: (min: 17, avg: 19.666666666666668, max: 24)\n",
      "\n",
      "Run: 14, exploration: 0.3507711574848344, score: 22\n",
      "Scores: (min: 17, avg: 21, max: 24)\n",
      "\n",
      "Run: 15, exploration: 0.33362200135903064, score: 11\n",
      "Scores: (min: 11, avg: 19, max: 24)\n",
      "\n",
      "Run: 16, exploration: 0.3141460853680822, score: 13\n",
      "Scores: (min: 11, avg: 15.333333333333334, max: 22)\n",
      "\n",
      "Run: 17, exploration: 0.29729358661854943, score: 12\n",
      "Scores: (min: 11, avg: 12, max: 13)\n",
      "\n",
      "Run: 18, exploration: 0.24328132378095624, score: 41\n",
      "Scores: (min: 12, avg: 22, max: 41)\n",
      "\n",
      "Run: 19, exploration: 0.19318370215794672, score: 47\n",
      "Scores: (min: 12, avg: 33.333333333333336, max: 47)\n",
      "\n",
      "Solved in 16 runs, 19 total runs.\n"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the code is running properly, you should begin to see output appearing above this code block. It will take several minutes, so it is recommended that you let this code run in the background while completing other work. When the code has finished, it will print output saying, \"Solved in _ runs, _ total runs.\"\n",
    "\n",
    "You may see an error about not having an exit command. This error does not affect the program's functionality and results from the steps taken to convert the code from Python 2.x to Python 3. Please disregard this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "\n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "  \n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.01    # changed learning rate from 0.001 to 0.01\n",
    "  \n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "  \n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 20\n",
      "Scores: (min: 20, avg: 20, max: 20)\n",
      "\n",
      "Run: 2, exploration: 0.9511101304657719, score: 11\n",
      "Scores: (min: 11, avg: 15.5, max: 20)\n",
      "\n",
      "Run: 3, exploration: 0.8955869907338783, score: 13\n",
      "Scores: (min: 11, avg: 14.666666666666666, max: 20)\n",
      "\n",
      "Run: 4, exploration: 0.7940753492934954, score: 25\n",
      "Scores: (min: 11, avg: 16.333333333333332, max: 25)\n",
      "\n",
      "Run: 5, exploration: 0.7514768435208588, score: 12\n",
      "Scores: (min: 12, avg: 16.666666666666668, max: 25)\n",
      "\n",
      "Run: 6, exploration: 0.7147372386831305, score: 11\n",
      "Scores: (min: 11, avg: 16, max: 25)\n",
      "\n",
      "Run: 7, exploration: 0.6797938283326578, score: 11\n",
      "Scores: (min: 11, avg: 11.333333333333334, max: 12)\n",
      "\n",
      "Run: 8, exploration: 0.6433260027715241, score: 12\n",
      "Scores: (min: 11, avg: 11.333333333333334, max: 12)\n",
      "\n",
      "Run: 9, exploration: 0.6149486215357263, score: 10\n",
      "Scores: (min: 10, avg: 11, max: 12)\n",
      "\n",
      "Run: 10, exploration: 0.5878229785513479, score: 10\n",
      "Scores: (min: 10, avg: 10.666666666666666, max: 12)\n",
      "\n",
      "Run: 11, exploration: 0.5562889678716474, score: 12\n",
      "Scores: (min: 10, avg: 10.666666666666666, max: 12)\n",
      "\n",
      "Run: 12, exploration: 0.4810273709480478, score: 30\n",
      "Scores: (min: 10, avg: 17.333333333333332, max: 30)\n",
      "\n",
      "Run: 13, exploration: 0.43296668905325736, score: 22\n",
      "Scores: (min: 12, avg: 21.333333333333332, max: 30)\n",
      "\n",
      "Run: 14, exploration: 0.41386834584198684, score: 10\n",
      "Scores: (min: 10, avg: 20.666666666666668, max: 30)\n",
      "\n",
      "Run: 15, exploration: 0.3877593341372176, score: 14\n",
      "Scores: (min: 10, avg: 15.333333333333334, max: 22)\n",
      "\n",
      "Run: 16, exploration: 0.3578751580867638, score: 17\n",
      "Scores: (min: 10, avg: 13.666666666666666, max: 17)\n",
      "\n",
      "Run: 17, exploration: 0.3420891339682016, score: 10\n",
      "Scores: (min: 10, avg: 13.666666666666666, max: 17)\n",
      "\n",
      "Run: 18, exploration: 0.322118930542046, score: 13\n",
      "Scores: (min: 10, avg: 13.333333333333334, max: 17)\n",
      "\n",
      "Run: 19, exploration: 0.3079101286968243, score: 10\n",
      "Scores: (min: 10, avg: 11, max: 13)\n",
      "\n",
      "Run: 20, exploration: 0.2943280830920294, score: 10\n",
      "Scores: (min: 10, avg: 11, max: 13)\n",
      "\n",
      "Run: 21, exploration: 0.2702863258025825, score: 18\n",
      "Scores: (min: 10, avg: 12.666666666666666, max: 18)\n",
      "\n",
      "Run: 22, exploration: 0.25578670228422234, score: 12\n",
      "Scores: (min: 10, avg: 13.333333333333334, max: 18)\n",
      "\n",
      "Run: 23, exploration: 0.24450384299593592, score: 10\n",
      "Scores: (min: 10, avg: 13.333333333333334, max: 18)\n",
      "\n",
      "Run: 24, exploration: 0.23489314109365644, score: 9\n",
      "Scores: (min: 9, avg: 10.333333333333334, max: 12)\n",
      "\n",
      "Run: 25, exploration: 0.22679417751985861, score: 8\n",
      "Scores: (min: 8, avg: 9, max: 10)\n",
      "\n",
      "Run: 26, exploration: 0.17388222158237718, score: 54\n",
      "Scores: (min: 8, avg: 23.666666666666668, max: 54)\n",
      "\n",
      "Run: 27, exploration: 0.1255322834371622, score: 66\n",
      "Scores: (min: 8, avg: 42.666666666666664, max: 66)\n",
      "\n",
      "Solved in 24 runs, 27 total runs.\n"
     ]
    }
   ],
   "source": [
    "# changed learning rate from 0.001 to 0.01\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "\n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "  \n",
    "GAMMA = 0.5     # changed discount factor from 0.95 to 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "  \n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "  \n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 16\n",
      "Scores: (min: 16, avg: 16, max: 16)\n",
      "\n",
      "Run: 2, exploration: 0.9000874278732445, score: 25\n",
      "Scores: (min: 16, avg: 20.5, max: 25)\n",
      "\n",
      "Run: 3, exploration: 0.8224322824348486, score: 19\n",
      "Scores: (min: 16, avg: 20, max: 25)\n",
      "\n",
      "Run: 4, exploration: 0.7666961448653229, score: 15\n",
      "Scores: (min: 15, avg: 19.666666666666668, max: 25)\n",
      "\n",
      "Run: 5, exploration: 0.6662995813682115, score: 29\n",
      "Scores: (min: 15, avg: 21, max: 29)\n",
      "\n",
      "Run: 6, exploration: 0.6149486215357263, score: 17\n",
      "Scores: (min: 15, avg: 20.333333333333332, max: 29)\n",
      "\n",
      "Run: 7, exploration: 0.5185893309484582, score: 35\n",
      "Scores: (min: 17, avg: 27, max: 35)\n",
      "\n",
      "Run: 8, exploration: 0.4932355662165453, score: 11\n",
      "Scores: (min: 11, avg: 21, max: 35)\n",
      "\n",
      "Run: 9, exploration: 0.43732904629000013, score: 25\n",
      "Scores: (min: 11, avg: 23.666666666666668, max: 35)\n",
      "\n",
      "Run: 10, exploration: 0.41386834584198684, score: 12\n",
      "Scores: (min: 11, avg: 16, max: 25)\n",
      "\n",
      "Run: 11, exploration: 0.3858205374665315, score: 15\n",
      "Scores: (min: 12, avg: 17.333333333333332, max: 25)\n",
      "\n",
      "Run: 12, exploration: 0.3614809303671764, score: 14\n",
      "Scores: (min: 12, avg: 13.666666666666666, max: 15)\n",
      "\n",
      "Run: 13, exploration: 0.3420891339682016, score: 12\n",
      "Scores: (min: 12, avg: 13.666666666666666, max: 15)\n",
      "\n",
      "Run: 14, exploration: 0.32864265128599696, score: 9\n",
      "Scores: (min: 9, avg: 11.666666666666666, max: 14)\n",
      "\n",
      "Run: 15, exploration: 0.3125753549412418, score: 11\n",
      "Scores: (min: 9, avg: 10.666666666666666, max: 12)\n",
      "\n",
      "Run: 16, exploration: 0.28704309604425327, score: 18\n",
      "Scores: (min: 9, avg: 12.666666666666666, max: 18)\n",
      "\n",
      "Run: 17, exploration: 0.27164454854530906, score: 12\n",
      "Scores: (min: 11, avg: 13.666666666666666, max: 18)\n",
      "\n",
      "Run: 18, exploration: 0.25578670228422234, score: 13\n",
      "Scores: (min: 12, avg: 14.333333333333334, max: 18)\n",
      "\n",
      "Run: 19, exploration: 0.08576489601717459, score: 219\n",
      "Scores: (min: 12, avg: 81.33333333333333, max: 219)\n",
      "\n",
      "Solved in 16 runs, 19 total runs.\n"
     ]
    }
   ],
   "source": [
    "# changed discount factor from 0.95 to 0.5\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "\n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "  \n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "  \n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "  \n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.5     # changed exploration decay from 0.995 to 0.5\n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 13\n",
      "Scores: (min: 13, avg: 13, max: 13)\n",
      "\n",
      "Run: 2, exploration: 0.01, score: 32\n",
      "Scores: (min: 13, avg: 22.5, max: 32)\n",
      "\n",
      "Run: 3, exploration: 0.01, score: 9\n",
      "Scores: (min: 9, avg: 18, max: 32)\n",
      "\n",
      "Run: 4, exploration: 0.01, score: 28\n",
      "Scores: (min: 9, avg: 23, max: 32)\n",
      "\n",
      "Run: 5, exploration: 0.01, score: 8\n",
      "Scores: (min: 8, avg: 15, max: 28)\n",
      "\n",
      "Run: 6, exploration: 0.01, score: 9\n",
      "Scores: (min: 8, avg: 15, max: 28)\n",
      "\n",
      "Run: 7, exploration: 0.01, score: 9\n",
      "Scores: (min: 8, avg: 8.666666666666666, max: 9)\n",
      "\n",
      "Run: 8, exploration: 0.01, score: 26\n",
      "Scores: (min: 9, avg: 14.666666666666666, max: 26)\n",
      "\n",
      "Run: 9, exploration: 0.01, score: 45\n",
      "Scores: (min: 9, avg: 26.666666666666668, max: 45)\n",
      "\n",
      "Run: 10, exploration: 0.01, score: 57\n",
      "Scores: (min: 26, avg: 42.666666666666664, max: 57)\n",
      "\n",
      "Solved in 7 runs, 10 total runs.\n"
     ]
    }
   ],
   "source": [
    "# changed exploration decay from 0.995 to 0.5\n",
    "cartpole()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e1138c4825c9d365a6bd9c48736751805c2b2e6b23e1e7f16feb2d7350124d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
