{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Augmenting training set images...\n",
      "Augment completed in: 0:07:02.797198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prest\\Desktop\\College_Programing\\CS-370-Current_Emerging_Trends_in_CS\\venv\\lib\\site-packages\\ipykernel_launcher.py:82: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "c:\\Users\\prest\\Desktop\\College_Programing\\CS-370-Current_Emerging_Trends_in_CS\\venv\\lib\\site-packages\\ipykernel_launcher.py:98: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=50, verbose=1, steps_per_epoch=390)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.9123 - accuracy: 0.3012\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.6422 - accuracy: 0.4073\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.5051 - accuracy: 0.4588\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.4191 - accuracy: 0.4907\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.3615 - accuracy: 0.5164\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 1.3199 - accuracy: 0.5306\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2866 - accuracy: 0.5439\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2598 - accuracy: 0.5518\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2428 - accuracy: 0.5562\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2231 - accuracy: 0.5681\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2081 - accuracy: 0.5769\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 1.1960 - accuracy: 0.58171s - loss: 1.1946 - accuracy: 0.\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1803 - accuracy: 0.5881\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 1.1829 - accuracy: 0.5878\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1770 - accuracy: 0.5926\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1750 - accuracy: 0.5914\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1656 - accuracy: 0.5959\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1561 - accuracy: 0.5998\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1535 - accuracy: 0.6013\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1557 - accuracy: 0.6008\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1509 - accuracy: 0.6006\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1471 - accuracy: 0.6033\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1546 - accuracy: 0.60000s - loss: 1.1546 - \n",
      "Epoch 24/50\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 1.1553 - accuracy: 0.6040\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1473 - accuracy: 0.6056\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1539 - accuracy: 0.60450s - loss: 1.1537 - accuracy: 0.60\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - 22s 55ms/step - loss: 1.1450 - accuracy: 0.6057\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1527 - accuracy: 0.6050\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1641 - accuracy: 0.6022\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1613 - accuracy: 0.6023\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1662 - accuracy: 0.6005\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1567 - accuracy: 0.6056\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1623 - accuracy: 0.6041\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1598 - accuracy: 0.6044\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1710 - accuracy: 0.5997\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1797 - accuracy: 0.6005\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - 22s 55ms/step - loss: 1.1761 - accuracy: 0.5989\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - 22s 57ms/step - loss: 1.1825 - accuracy: 0.6001\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1850 - accuracy: 0.5961\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1931 - accuracy: 0.5939\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1953 - accuracy: 0.5925\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.1927 - accuracy: 0.5952\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2064 - accuracy: 0.5920\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2061 - accuracy: 0.5922\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2148 - accuracy: 0.5900\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2238 - accuracy: 0.5826\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2251 - accuracy: 0.5874\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2402 - accuracy: 0.5815\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2277 - accuracy: 0.5849\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - 22s 56ms/step - loss: 1.2415 - accuracy: 0.5796\n",
      "10000/10000 [==============================] - 1s 86us/step\n",
      "Test score: 1.0497670904159546\n",
      "Test accuracy: 0.6535999774932861\n",
      "Total time: 0:25:19.645988\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 50\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_TO_AUGMENT=5\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# augumenting\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(rotation_range=40, \n",
    "                            width_shift_range=0.2, \n",
    "                            height_shift_range=0.2, \n",
    "                            zoom_range=0.2, \n",
    "                            horizontal_flip=True, \n",
    "                            fill_mode='nearest')\n",
    "\n",
    "\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)\n",
    "    for x_aug in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "        if num_aug >= NUM_TO_AUGMENT:\n",
    "            break\n",
    "        xtas.append(x_aug[0])\n",
    "        num_aug += 1\n",
    "\n",
    "print(\"Augment completed in: \" + str(datetime.now() - startTime))\n",
    "\n",
    "#fit the dataget\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "\n",
    "#AUGMENTATION MODEL TRAINING\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0], epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "\n",
    "#NON-AUGMENTATION MODEL TRAINING\n",
    "#model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str(datetime.now() - startTime))\n",
    "\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel that an algorithm like the one we used for this assignment could easily be trained for things like facial recognition. In fact, facial recognition algorithms already exist in today’s day and age. While they can be used for good things such as locating criminals, human trafficking victims, etc. there is the fact that these types of algorithms can be used for some not so good things. Would you like it if your every move was being tracked and logged? Imagine if you were walking home one day and there were no cars on the road. You decide to cross the road while there are no cars rather than walk further up and wait at the crosswalk. This would be considered jaywalking and is a fine-able offence in many states. But no cars were present, so is it that big of a deal? It might be to you when you get a fine in the mail because a camera using a facial recognition algorithm identified you as you were committing the infraction. There’s actually a country where something like this has already been implemented, and that place is China. Alfred Ng from CNET writes “Cameras set up at crosswalks to identify and post photos of jaywalkers are commonplace, and a report from Abacus in May 2019 showed photos of children jaywalking on a digital billboard. The local traffic police said \"children should be treated the same as adults,\" according to the outlet.” (Ng, 2020). This may not seem like that much of an issue as the whole reason jaywalking laws exist is to help keep pedestrians safe, using facial recognition software in this manner also requires extensive data collection. I would argue that most people would prefer it if their government didn’t track their every move. What if a government used the data collected to commit racism or persecute a minority population? Most people would agree that this is the exact opposite of what facial recognition software should be used for. Well, the Chinese government doesn’t agree. “…documents and interviews show that the authorities are also using a vast, secret system of advanced facial recognition technology to track and control the Uighurs, a largely Muslim minority. It is the first known example of a government intentionally using artificial intelligence for racial profiling, experts said.” (Mozur, 2019). This is a demonstration of what is probably the primary ethical/privacy concern when it comes to image/facial recognition algorithms. While they can be used for good, they can also very easily be used for bad. \n",
    "\n",
    "References\n",
    "\n",
    "Mozur, P. (2019, April 14). One Month, 500,000 Face Scans: How China Is Using A.I. to Profile a Minority. Retrieved from The New York Times: https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html\n",
    "\n",
    "Ng, A. (2020, August 11). How China uses facial recognition to control human behavior. Retrieved from CNET: https://www.cnet.com/news/politics/in-china-facial-recognition-public-shaming-and-control-go-hand-in-hand/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1138c4825c9d365a6bd9c48736751805c2b2e6b23e1e7f16feb2d7350124d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
