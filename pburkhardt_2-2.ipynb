{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_239 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 1.4982 - accuracy: 0.6160 - val_loss: 0.7603 - val_accuracy: 0.8422\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.5970 - accuracy: 0.8563 - val_loss: 0.4521 - val_accuracy: 0.8830\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.4350 - accuracy: 0.8835 - val_loss: 0.3722 - val_accuracy: 0.8981\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.3748 - accuracy: 0.8958 - val_loss: 0.3352 - val_accuracy: 0.9050\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.3410 - accuracy: 0.9048 - val_loss: 0.3100 - val_accuracy: 0.9136\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.3178 - accuracy: 0.9102 - val_loss: 0.2926 - val_accuracy: 0.9166\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2998 - accuracy: 0.9146 - val_loss: 0.2780 - val_accuracy: 0.9211\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2852 - accuracy: 0.9191 - val_loss: 0.2660 - val_accuracy: 0.9245\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2731 - accuracy: 0.9221 - val_loss: 0.2560 - val_accuracy: 0.9283\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2622 - accuracy: 0.9254 - val_loss: 0.2488 - val_accuracy: 0.9287\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.2523 - accuracy: 0.9278 - val_loss: 0.2398 - val_accuracy: 0.9317\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2431 - accuracy: 0.9305 - val_loss: 0.2323 - val_accuracy: 0.9339\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.2348 - accuracy: 0.9326 - val_loss: 0.2280 - val_accuracy: 0.9355\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2274 - accuracy: 0.9349 - val_loss: 0.2198 - val_accuracy: 0.9374\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.2200 - accuracy: 0.9373 - val_loss: 0.2128 - val_accuracy: 0.9392\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2134 - accuracy: 0.9385 - val_loss: 0.2074 - val_accuracy: 0.9415\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.2071 - accuracy: 0.9403 - val_loss: 0.2020 - val_accuracy: 0.9427\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2011 - accuracy: 0.9428 - val_loss: 0.1975 - val_accuracy: 0.9434\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.1956 - accuracy: 0.9439 - val_loss: 0.1938 - val_accuracy: 0.9454\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1905 - accuracy: 0.9451 - val_loss: 0.1885 - val_accuracy: 0.9463\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "Test score: 0.18818295242637395\n",
      "Test accuracy: 0.9467999935150146\n",
      "Total time: 0:00:35.076000\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_248 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.7455 - accuracy: 0.8077 - val_loss: 0.3395 - val_accuracy: 0.9064\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.3243 - accuracy: 0.9079 - val_loss: 0.2728 - val_accuracy: 0.9208\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2704 - accuracy: 0.9228 - val_loss: 0.2384 - val_accuracy: 0.9320\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.2366 - accuracy: 0.9322 - val_loss: 0.2134 - val_accuracy: 0.9396\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2112 - accuracy: 0.9388 - val_loss: 0.1942 - val_accuracy: 0.9459\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.1913 - accuracy: 0.9452 - val_loss: 0.1811 - val_accuracy: 0.9481\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.1739 - accuracy: 0.9497 - val_loss: 0.1692 - val_accuracy: 0.9511\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.1587 - accuracy: 0.9541 - val_loss: 0.1581 - val_accuracy: 0.9542\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.1465 - accuracy: 0.9581 - val_loss: 0.1450 - val_accuracy: 0.9584\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.1352 - accuracy: 0.9616 - val_loss: 0.1391 - val_accuracy: 0.9605\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1252 - accuracy: 0.9643 - val_loss: 0.1324 - val_accuracy: 0.9626\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.1164 - accuracy: 0.9661 - val_loss: 0.1270 - val_accuracy: 0.9631\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1090 - accuracy: 0.9688 - val_loss: 0.1288 - val_accuracy: 0.9627\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.1022 - accuracy: 0.9708 - val_loss: 0.1190 - val_accuracy: 0.9647\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.0956 - accuracy: 0.9729 - val_loss: 0.1133 - val_accuracy: 0.9669\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.0903 - accuracy: 0.9740 - val_loss: 0.1099 - val_accuracy: 0.9687\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.0849 - accuracy: 0.9762 - val_loss: 0.1052 - val_accuracy: 0.9697\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.0803 - accuracy: 0.9772 - val_loss: 0.1033 - val_accuracy: 0.9691\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.0759 - accuracy: 0.9790 - val_loss: 0.1026 - val_accuracy: 0.9702\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.0721 - accuracy: 0.9804 - val_loss: 0.1007 - val_accuracy: 0.9697\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "Test score: 0.09466224124152213\n",
      "Test accuracy: 0.9707000255584717\n",
      "Total time: 0:02:01.575995\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_251 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 1.8665 - accuracy: 0.4834 - val_loss: 1.3338 - val_accuracy: 0.7286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 1.0110 - accuracy: 0.7769 - val_loss: 0.7353 - val_accuracy: 0.8384\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.6598 - accuracy: 0.8372 - val_loss: 0.5383 - val_accuracy: 0.8683\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.5245 - accuracy: 0.8643 - val_loss: 0.4497 - val_accuracy: 0.8830\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4553 - accuracy: 0.8789 - val_loss: 0.4005 - val_accuracy: 0.8944\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4131 - accuracy: 0.8881 - val_loss: 0.3695 - val_accuracy: 0.8992\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3844 - accuracy: 0.8942 - val_loss: 0.3468 - val_accuracy: 0.9041\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3632 - accuracy: 0.8993 - val_loss: 0.3302 - val_accuracy: 0.9080\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3470 - accuracy: 0.9035 - val_loss: 0.3174 - val_accuracy: 0.9109\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3337 - accuracy: 0.9066 - val_loss: 0.3071 - val_accuracy: 0.9128\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3225 - accuracy: 0.9087 - val_loss: 0.2976 - val_accuracy: 0.9154\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3127 - accuracy: 0.9111 - val_loss: 0.2897 - val_accuracy: 0.9184\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3041 - accuracy: 0.9139 - val_loss: 0.2841 - val_accuracy: 0.9184\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2966 - accuracy: 0.9157 - val_loss: 0.2765 - val_accuracy: 0.9208\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2896 - accuracy: 0.9178 - val_loss: 0.2703 - val_accuracy: 0.9227\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2831 - accuracy: 0.9196 - val_loss: 0.2650 - val_accuracy: 0.9240\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2772 - accuracy: 0.9211 - val_loss: 0.2602 - val_accuracy: 0.9243\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2716 - accuracy: 0.9230 - val_loss: 0.2558 - val_accuracy: 0.9261\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2665 - accuracy: 0.9240 - val_loss: 0.2513 - val_accuracy: 0.9271\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2618 - accuracy: 0.9253 - val_loss: 0.2475 - val_accuracy: 0.9284\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "Test score: 0.25154674277305605\n",
      "Test accuracy: 0.9277999997138977\n",
      "Total time: 0:00:19.938010\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imp\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 256\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "stopTime = datetime.now()\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Total time: \" + str((stopTime - startTime)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first run, we can see that by the last epoch, the training accuracy is 94.5% and the validation accuracy is 94.6%. The overall test accuracy is 94.6% as well. The parameter I chose to change is the batch size. Our text, Deep Learning with Keras, tells us that batch size is “the number of training instances observed before the optimizer performs a weight update.” (Gulli & Pal, 2017). The code given to us sets this parameter at 128. This means that 128 pieces of data are fed to the algorithm and then the weights of the node branches are updated based on the results that those 128 data pieces generate. I decided first to reduce this number to see what effect it would have on the accuracy of the model. I set it to 1/4th of its original value, resulting in a batch size of 32. This will result in the model updating the node branch weights more often during execution, but hopefully also result in a more accurate model. After running this model, we can see that the change in batch size did have a positive effect on the accuracy of the model! The training and validation accuracies were 98.0% and 96.9%, while the test accuracy was 97.0%. So, the accuracy got better, but there is one other thing to note. The validation score is lower than the test score. This can be a sign of overfitting if these scores differ greatly. Overfitting is when the model is very good at categorizing the training data, but not as good categorizing data that it hasn’t seen before. While the accuracy scores increased, we want to be careful since the whole purpose of the model is to train it so that it can perform as well or better on data it hasn’t seen before.\n",
    "\n",
    "For the third run, I wanted to see what happens if we double the batch size. My prediction was that the model would run faster, but be less accurate. Sure enough, the model was quicker, but not as accurate as both the first and second run. The test and validation accuracies were 92.5% and 92.8% while the test accuracy was 92.7%. This is due to the node branch weights not getting updated as often. It’s like if you were driving somewhere in your car and you checked a map every 50 miles to make sure you’re going the right direction. You won’t be as accurate when it comes to reaching your destination, but you will get there quicker due to less stops. Whereas if you check the map every 5 miles, it will take you longer to get there, but also the likelihood of you making a wrong turn is greatly reduced (more accuracy).\n",
    "\n",
    "References\n",
    "Gulli, A., & Pal, S. (2017). Deep Learning with Keras. Birmingham: Packt Publishing Ltd.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d020614f9a2dc57296da84a5f935b826668275142c238b2790968ef78e3430c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
